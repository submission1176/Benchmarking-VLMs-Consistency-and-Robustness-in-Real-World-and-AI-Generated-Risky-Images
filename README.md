# Benchmarking VLMs Consistency and Robustness in Real-World and AI-Generated Risky Images


## Abstract
The rapid advancements in Text-to-Image (T2I) generative models have led to an exponential increase in visual media. However, the resulting proliferation of online visual content, encompassing both generated and real-world images, raises significant safety concerns particularly for the underage Internet users.To address this critical safety detection challenge, we investigate the efficacy of Vision-Language Models (VLMs) as scalable content moderators for web ecosystems. We conduct a comprehensive robustness evaluation of four state-of-the-art VLMs. This evaluation comprises two key components. Visual style transformation tests the modelsâ€™ capacity to maintain safety classification performance when the visual style of the images is transformed, and task reformulation provides a detailed analysis of VLM robustness in semantic understanding. Our findings reveal significant weaknesses and a lack of robustness in these leading VLMs across both visual and textual encodings. These results indicate that current safety detection mechanisms for visual
media require substantial improvement and highlight a critical area for future research and development in content moderation technology. 
$${\color{red}Warning:  \space This  \space paper  \space contains  \space examples  \space that  \space may  \space be  \space offensive  \space or  \space harmful  \space in \space  nature.}$$
